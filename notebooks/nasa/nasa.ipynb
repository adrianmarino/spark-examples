{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Nasa example\n",
    "\n",
    "**Question**: What are the endpoints most accessed between 20:00 and 23:59?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Before all, download [nasa logs](http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Step 2**: Install apache-log-parser lib:\n",
    "```bash\n",
    "pip install apache-log-parser\n",
    "```\n",
    "After installation reboot jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Set max executor memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f248222efd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.set(\"spark.executor.memory\", \"8g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Load logs and show lines count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Line: ['199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245']\n",
      "File size: 1891715 lines.\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('/home/adrian/development/spark/notebooks/nasa/access_log_Jul95').cache()\n",
    "\n",
    "print('First Line: %s' % lines.take(1))\n",
    "print('File size: %s lines.' % lines.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Get log lines in tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line tuple: [('199.72.81.55', datetime.datetime(1995, 7, 1, 0, 0, 1), 'GET /history/apollo/ HTTP/1.0', 200, 6245)].\n"
     ]
    }
   ],
   "source": [
    "import apache_log_parser\n",
    "parser = apache_log_parser.make_parser('%h - - %t \\\"%r\\\" %s %b')\n",
    "\n",
    "# print(parser('unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985'))\n",
    "\n",
    "line_tuples = lines.map(parser).map(lambda line: (line['remote_host'], \\\n",
    "                                                  line['time_received_datetimeobj'], \\\n",
    "                                                  line['request_first_line'], \\\n",
    "                                                  int(line['status']), \\\n",
    "                                                  int(line['response_bytes_clf'].replace('-','0'))) \\\n",
    "                                   ).cache()\n",
    "\n",
    "print('Line tuple: %s.' % line_tuples.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Get log rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+------+-------------+\n",
      "| remote_host|           timestamp|             request|status|response_size|\n",
      "+------------+--------------------+--------------------+------+-------------+\n",
      "|199.72.81.55|1995-07-01 00:00:...|GET /history/apol...|   200|         6245|\n",
      "+------------+--------------------+--------------------+------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "LogRow = StructType([  StructField('remote_host',   StringType(),    False), \\\n",
    "                       StructField('timestamp',     TimestampType(), False), \\\n",
    "                       StructField('request',       StringType(),    False), \\\n",
    "                       StructField('status',        IntegerType(),   False), \\\n",
    "                       StructField('response_size', LongType(),      False), ])\n",
    "\n",
    "rows = sqlCtx.createDataFrame(line_tuples, LogRow).cache()\n",
    "\n",
    "rows.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Get rows between 20:00 and 23:59."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In Progress..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Get endpoints most accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In Progress..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
