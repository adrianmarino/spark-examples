{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skapespeare example\n",
    "\n",
    "**Step 1**: After all search \"shakespeare.txt\" on google, download first link and rename to skapespeare.txt.\n",
    "\n",
    "**Step 2**: Load skakespeare complete works to spark context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"/home/adrian/development/spark/notebooks/shakespeare/shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Get words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split(' ')) \\\n",
    "                .map(lambda word: word.lower()) \\\n",
    "                .filter(lambda word: len(word) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**: Remove stop words and puctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sonnets', 'william', 'shakespeare', 'fairest', 'creatures', 'desire', 'increase', 'beautys', 'rose', 'die']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def strip_punctuation(string):\n",
    "    return ''.join(char for char in string if char not in punctuation)\n",
    "\n",
    "stop_words = [\"i\", \"you\", \"he\", \"she\", \"it\", \"they\", \"we\", \"us\", \"them\", \"him\", \"her\", \"my\", \"your\", \"our\", \"his\", \"its\", \"their\", \"mine\", \"ours\", \"yours\", \"theirs\", \"hers\", \"me\", \"ii\",\"iii\",\"iv\",\"vi\",\"vii\",\"viii\",\"ix\",\"x\",\"xi\",\"xii\",\"xiii\",\"xiv\",\"xv\",\"xvi\",\"xvii\",\"xvii\",\"xviii\",\"xix\",\"xx\", \"one\", \"two\", \"thee\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"will\", \"thy\", \"a\",\"able\",\"about\",\"above\",\"abst\",\"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\"affects\",\"after\",\"afterwards\",\"again\",\"against\",\"ah\",\"all\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"announce\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\"approximately\",\"are\",\"aren\",\"arent\",\"arise\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"at\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"between\",\"beyond\",\"biol\",\"both\",\"brief\",\"briefly\",\"but\",\"by\",\"c\",\"ca\",\"came\",\"can\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"could\",\"couldnt\",\"d\",\"date\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"done\",\"don't\",\"down\",\"downwards\",\"due\",\"during\",\"e\",\"each\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\"especially\",\"et\",\"et-al\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\"far\",\"few\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"gave\",\"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"had\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"hed\",\"hence\",\"her\",\"here\",\"hereafter\",\"hereby\",\"herein\",\"heres\",\"hereupon\",\"hers\",\"herself\",\"hes\",\"hi\",\"hid\",\"him\",\"himself\",\"his\",\"hither\",\"home\",\"how\",\"howbeit\",\"however\",\"hundred\",\"i\",\"id\",\"ie\",\"if\",\"i'll\",\"im\",\"immediate\",\"immediately\",\"importance\",\"important\",\"in\",\"inc\",\"indeed\",\"index\",\"information\",\"instead\",\"into\",\"invention\",\"inward\",\"is\",\"isn't\",\"it\",\"itd\",\"it'll\",\"its\",\"itself\",\"i've\",\"j\",\"just\",\"k\",\"keep\tkeeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"means\",\"meantime\",\"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\"must\",\"my\",\"myself\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\"normally\",\"nos\",\"not\",\"noted\",\"nothing\",\"now\",\"nowhere\",\"o\",\"obtain\",\"obtained\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"omitted\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"ord\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"owing\",\"own\",\"p\",\"page\",\"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"poorly\",\"possible\",\"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"re\",\"readily\",\"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\"respectively\",\"resulted\",\"resulting\",\"results\",\"right\",\"run\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"she\",\"shed\",\"she'll\",\"shes\",\"should\",\"shouldn't\",\"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significant\",\"significantly\",\"similar\",\"similarly\",\"since\",\"six\",\"slightly\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"such\",\"sufficiently\",\"suggest\",\"sup\",\"sure\tt\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that'll\",\"thats\",\"that've\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"these\",\"they\",\"theyd\",\"they'll\",\"theyre\",\"they've\",\"think\",\"this\",\"those\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"through\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"very\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasnt\",\"way\",\"we\",\"wed\",\"welcome\",\"we'll\",\"went\",\"were\",\"werent\",\"we've\",\"what\",\"whatever\",\"what'll\",\"whats\",\"when\",\"whence\",\"whenever\",\"where\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whim\",\"whither\",\"who\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whom\",\"whomever\",\"whos\",\"whose\",\"why\",\"widely\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"would\",\"wouldnt\",\"www\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"youd\",\"you'll\",\"your\",\"youre\",\"yours\",\"yourself\",\"yourselves\",\"you've\",\"z\",\"zero\"]\n",
    "\n",
    "words = words.map(lambda word: strip_punctuation(word).strip()) \\\n",
    "             .filter(lambda word: not (word in stop_words or word.isdigit())) \\\n",
    "             .cache()\n",
    "\n",
    "print(words.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5**: Get more popular words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lord', 3059), ('king', 2861), ('good', 2812), ('sir', 2754), ('well', 2462), ('enter', 2098), ('love', 2054), ('ill', 1972), ('hath', 1941), ('man', 1835)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def sum():\n",
    "    return lambda a,b: a + b\n",
    "\n",
    "def descendent():\n",
    "    return lambda word_times: -word_times[1]\n",
    "\n",
    "words_frequency = words.map(lambda word: (word, 1)).reduceByKey(sum()).cache()\n",
    "\n",
    "print(words_frequency.takeOrdered(10, descendent()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6**: Get diferent words count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferent words count: 27441\n"
     ]
    }
   ],
   "source": [
    "print(\"Diferent words count: \" + str(words.distinct().count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7**: Get most frequent two letters words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ay', 764), ('ye', 289), ('ha', 217), ('ho', 195), ('em', 167), ('de', 115), ('la', 77), ('lo', 74), ('le', 60), ('je', 28)]\n"
     ]
    }
   ],
   "source": [
    "two_letter_words_frequency = words_frequency.filter(lambda word_times: len(word_times[0]) == 2) \\\n",
    "                                            \n",
    "print(two_letter_words_frequency.takeOrdered(10, descendent()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
